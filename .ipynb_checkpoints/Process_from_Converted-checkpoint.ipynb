{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4390b86-cef7-4d98-a2ed-037a17faaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd4af542-7e23-497f-b46c-a39e6a6a3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fast_processing_engine():\n",
    "    \"\"\"Class to take raw fast data from a given site and turn it into a standardized product.\"\"\"\n",
    "    \n",
    "    def __init__(self, converted_dirs, metadata_fn, file_length, acq_freq, start_time, end_time, site_names, out_dir):\n",
    "        \"\"\"converted_dirs: list of directories containing raw converted TOA5 files, as strings. If multiple directories are provided, they will be combined into one output directory\n",
    "        metadata_dir: string giving the filename of the metadata .xlsl file\n",
    "        file_length: int, file length in minutes\n",
    "        acq_freq: int, acquisition speed in Hz\n",
    "        start_time, end_time: the start and end dates for the files to process in yyyy-mm-dd hh:mm format. Round DOWN to the nearest half-hour.\n",
    "        site_names: list of site names to combine. Must be in the same order as converted_dir\n",
    "        out_dir: str, output/processing directory.\"\"\"\n",
    "        \n",
    "        header_sheets = {\n",
    "                \"BB-NF17\": \"NF 17m 10Hz Code\",\n",
    "                \"BB-NF3\": \"NF 3m 10Hz Code\"\n",
    "        }\n",
    "        \n",
    "        final_headers = {\n",
    "            \"BB-NF17\": [\"TIMESTAMP\", \"Ux_CSAT3_NF17\", \"Uy_CSAT3_NF17\", \"Uz_CSAT3_NF17\", \"Ts_CSAT3_NF17\", \"Ux_CSAT3_NF7\", \"Uy_CSAT3_NF7\", \"Uz_CSAT3_NF7\", \"Ts_CSAT3_NF7\", \"rho_c_LI7500_NF17\", \"rho_v_LI7500_NF17\", \"DIAG_CSAT3_NF17\", \"DIAG_CSAT3_NF17\", \"DIAG_LI7500_NF17\"],\n",
    "            \"BB-NF3\": [\"TIMESTAMP\", \"Ux_CSAT3B_NF3\", \"Uy_CSAT3B_NF3\", \"Uz_CSAT3B_NF3\", \"Ts_CSAT3B_NF3\", \"rho_c_LI7500_NF3\", \"rho_v_LI7500_NF3\", \"DIAG_CSAT3_NF3\", \"DIAG_LI7500_NF3\"]\n",
    "        }\n",
    "        self.site_info = {site:{\"fns\":None, \n",
    "                                \"file_tss\":None, \n",
    "                                \"desired_file_tss\":None, \n",
    "                                \"converted_path\":Path(converted_dir), \n",
    "                                \"n_files_converted\":None,\n",
    "                                \"rawfile_metadata\":None,\n",
    "                                \"header_sheet\":header_sheets[site],\n",
    "                                \"header_metadat\":None\n",
    "                                \"final_header\":final_headers[site]\n",
    "                               }\n",
    "                         for site, converted_dir in zip(site_names, converted_dirs)}\n",
    "        \n",
    "        self.start_time = pd.to_datetime(start_time)\n",
    "        self.end_time = pd.to_datetime(end_time)\n",
    "\n",
    "        # convert acq freq to interval\n",
    "        self.acq_freq = acq_freq\n",
    "        self.acq_period = pd.Timedelta(f'{1000//self.acq_freq} ms')\n",
    "    \n",
    "        self.file_length = file_length\n",
    "        self.n_records = self.file_length*self.acq_freq*60\n",
    "\n",
    "        # convert files and directories to path objects and create an output directory\n",
    "        self.out_path = Path(out_dir)\n",
    "        self.metadata_path = Path(metadata_fn)\n",
    "        \n",
    "        if not self.out_path.exists():\n",
    "            self.out_path.mkdir()\n",
    "        return\n",
    "    \n",
    "    def get_timestamp_from_fn(self, fn):\n",
    "        \"\"\"given a fast file, this will extract the timestamp in its name\"\"\"\n",
    "        file_id = Path(fn).name.split('Hz')[1]\n",
    "        file_start_str = \"\".join(re.split(\"_|\\.\", file_id)[1:-1])\n",
    "        file_start_ts = pd.to_datetime(file_start_str, format=\"%Y%m%d%H%M\")\n",
    "        return file_start_ts\n",
    "    \n",
    "    def get_fn_from_timestamp(self, file_start_ts, site):\n",
    "        \"\"\"given a timestamp, this will find the exact file name it's associated with\"\"\"\n",
    "        file_start_str = (f'{file_start_ts.year:04d}_' + \n",
    "                          f'{file_start_ts.month:02d}_' + \n",
    "                          f'{file_start_ts.day:02d}_' + \n",
    "                          f'{file_start_ts.hour:02d}{file_start_ts.minute:02d}')\n",
    "\n",
    "        # if the file exists\n",
    "        for i, fn in enumerate(self.site_info[site]['fns']):\n",
    "            if file_start_str in str(fn):\n",
    "                return fn\n",
    "\n",
    "        # if not\n",
    "        return\n",
    "    \n",
    "    def find_fast_files(self):\n",
    "        \"\"\"find all the raw data files that the user wants to process and place them in the site info dict\"\"\"\n",
    "        \n",
    "        for site in self.site_info:\n",
    "            # get the timestamps we want to see\n",
    "            self.site_info[site]['desired_file_tss'] = pd.date_range(self.start_time, self.end_time, freq=f'{self.file_length} min')\n",
    "            \n",
    "            # retrieve the raw file names\n",
    "            self.site_info[site]['fns'] = list(self.site_info[site]['converted_path'].glob(\"TOA5*.dat\"))\n",
    "            \n",
    "            # get raw file timestamps\n",
    "            file_tss = []\n",
    "            for fn in self.site_info[site]['fns']:\n",
    "                fts = self.get_timestamp_from_fn(fn)\n",
    "                if (fts >= self.start_time and fts <= self.end_time):\n",
    "                    file_tss.append(self.get_timestamp_from_fn(fn))\n",
    "            \n",
    "            # sort file names by timestamp\n",
    "            file_tss = sorted(file_tss)\n",
    "            self.site_info[site]['fns'] = [self.get_fn_from_timestamp(fts, site) for fts in file_tss]\n",
    "            self.site_info[site]['file_tss'] = file_tss.copy()\n",
    "            \n",
    "            self.site_info[site]['n_files_converted'] = len(file_tss)\n",
    "      \n",
    "    def metadata_template(self):\n",
    "        \"\"\"create a blank template to store raw file metadata in\"\"\"\n",
    "        \n",
    "        for site in self.site_info:\n",
    "            rawfile_metadata = pd.DataFrame(\n",
    "                data=np.zeros((self.site_info[site]['n_files_converted'], 8), dtype=str),\n",
    "                columns=['Encoding', 'Station_name', 'Datalogger_model', 'Datalogger_serial_number', 'Datalogger_OS_version', 'Datalogger_program_name', 'Datalogger_program_signature', 'Table_name']\n",
    "            )\n",
    "            rawfile_metadata['File_name'] = self.site_info[site]['fns']\n",
    "            rawfile_metadata['TIMESTAMP'] = self.site_info[site]['file_tss']\n",
    "            rawfile_metadata['out_fn'] = np.NAN\n",
    "            rawfile_metadata.set_index('TIMESTAMP', inplace=True)\n",
    "\n",
    "            self.site_info[site]['rawfile_metadata'] = rawfile_metadata.copy()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def summary_template(self, header):\n",
    "        # initialize summary statistics array: [TIMESTAMP, RECORD, Ux_Max, Ux_Min, Ux_Std, ... rho_c_Max, ...]\n",
    "        summary_header = []\n",
    "        for colname in header[2:]:\n",
    "            summary_header.append(colname + '_Max')\n",
    "            summary_header.append(colname + '_Min')\n",
    "            summary_header.append(colname + '_Std')\n",
    "            summary_header.append(colname + '_Mean')\n",
    "            summary_header.append(colname + '_NANPct')\n",
    "        summary_data = np.empty((len(self.desired_file_tss), len(summary_header)))\n",
    "        return summary_data, summary_header\n",
    "        \n",
    "    \n",
    "    def get_fast_header_info(self):\n",
    "        \"\"\"each site has a specially formatted header associated with it\"\"\"\n",
    "        \n",
    "        for site in self.site_info:\n",
    "            print(site)\n",
    "            metadat = pd.read_excel(self.metadata_path, sheet_name = self.site_info[site]['header_sheet'])\n",
    "            metadat['Date_yyyymmdd'] = pd.to_datetime(metadat['Date_yyyymmdd'], format='%Y%m%d')\n",
    "            self.site_info[site]['header_metadat'] = metadat.copy()\n",
    "        return\n",
    "    \n",
    "    def reorder_headers(self, site, date):\n",
    "        \"\"\"given a site name and a date, this will return the indices to rearrange the headers at. \n",
    "        \n",
    "        For example, if the raw file has the header\n",
    "        a, b, c, e, g, d, f\n",
    "        \n",
    "        and the final file should have the header\n",
    "        \n",
    "        a, b, c, d, e, f, g\n",
    "        \n",
    "        then this will return\n",
    "        \n",
    "        [0, 1, 2, 4, 6, 3, 5]\"\"\"\n",
    "        \n",
    "        if site == 'BB-NF17':\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def standardize_fast_files(self):\n",
    "        \"\"\"reads in raw fast files, and combines/standardizes them to be continuous. Re-writes the files. Also computes diagnostic/summary statistics on the data\"\"\"\n",
    "        \n",
    "        for site in site_info:\n",
    "            # initialize summary data\n",
    "            summary_data, summary_header = self.summary_template(self.site_info[site]['final_header'])\n",
    "\n",
    "            # we'll be popping file names off of fns and file_tss, so we'll create temporary copies of them first\n",
    "            fns_temp, file_tss_temp = self.site_info[site]['fns'].copy(), self.site_info[site]['file_tss'].copy()\n",
    "\n",
    "            # start processing files by desired start timestamp:\n",
    "            # for each file time window (usually every half-hour), find all the converted raw files that will fit into that time window.\n",
    "            # then, if there are any holes in the timeseries defined by those raw files, fill the missing timestamps and then fill the missing data\n",
    "            # with NANs. Write the final file to a csv and report file summary statistics.\n",
    "            for idfts, dfts in enumerate(tqdm(self.site_info[site]['desired_file_tss'])):\n",
    "                \n",
    "                # generate the desired time index of the input file (start 100ms after file timestamp)\n",
    "                desired_time_index = pd.date_range(dfts + self.acq_period, periods=self.n_records, freq=self.acq_period)\n",
    "                dat = pd.DataFrame(desired_time_index, columns=['TIMESTAMP'])\n",
    "                dat.set_index('TIMESTAMP', inplace=True)\n",
    "\n",
    "                # find qualified files for this time interval\n",
    "                next_file_ts = dfts + pd.Timedelta(f'{self.file_length} Min')\n",
    "                next_fns, next_file_tss = [], []\n",
    "                try:\n",
    "                    while file_tss_temp[0] < next_file_ts:\n",
    "                        next_fns.append(fns_temp.pop(0))\n",
    "                        next_file_tss.append(file_tss_temp.pop(0))\n",
    "                except IndexError as err:\n",
    "                    pass\n",
    "\n",
    "                # combine the found files\n",
    "                # if no valid files are found, just make a null dataframe\n",
    "                if next_fns == []:\n",
    "                    rawdat = pd.DataFrame(np.full((1, len(self.site_info[site]['final_header'])), np.nan), \n",
    "                                          columns=self.site_info[site]['final_header'])\n",
    "                    rawdat['TIMESTAMP'] = desired_time_index[0]\n",
    "                    rawdat.set_index('TIMESTAMP', inplace=True)\n",
    "                    \n",
    "                # otherwise proceed as normal\n",
    "                else:\n",
    "                    for i, fn, ts in zip(range(len(next_fns)), next_fns, next_file_tss):\n",
    "                        if i == 0:\n",
    "                            rawdat = pd.read_csv(fn, sep=',', header=[0], skiprows=[0, 2, 3], na_values = ['NAN', '-4400906'])\n",
    "                            rawdat['TIMESTAMP'] = pd.to_datetime(rawdat['TIMESTAMP'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                            rawdat.set_index('TIMESTAMP', inplace=True)\n",
    "                        else:\n",
    "                            rawdat_tmp = pd.read_csv(fn, sep=',', header=[0], skiprows=[0, 2, 3], na_values = ['NAN', '-4400906'])\n",
    "                            rawdat_tmp['TIMESTAMP'] = pd.to_datetime(rawdat_tmp['TIMESTAMP'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                            rawdat_tmp.set_index('TIMESTAMP', inplace=True)\n",
    "                            rawdat = pd.concat([rawdat, rawdat_tmp])\n",
    "                            rawdat = rawdat.merge(rawdat_tmp, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "                        # get the metadata\n",
    "                        dfts_str = re.sub('-| ', '_', str(dfts))\n",
    "                        dfts_str = re.sub(':', '', dfts_str)[:-2]\n",
    "                        desired_fn = out_path / f'{dfts_str}.csv'\n",
    "                        with open(fn) as f: self.rawfile_metadata.loc[ts] = f.readline()[1:-2].split('\",\"') + [fn, desired_fn]\n",
    "\n",
    "                # merge raw files into complete array\n",
    "                dat = dat.merge(rawdat, how='outer', left_index=True, right_index=True, sort=True)\n",
    "                self.write_out(dfts, dat)\n",
    "\n",
    "\n",
    "                # write summary stats\n",
    "                for icolname, colname in enumerate(dat.columns[2:]):\n",
    "                    summary_row = np.array([\n",
    "                        np.nanmax(dat[colname]),\n",
    "                        np.nanmin(dat[colname]),\n",
    "                        np.nanstd(dat[colname]),\n",
    "                        np.nanmean(dat[colname]),\n",
    "                        stats.skew(dat[colname], nan_policy='omit'),\n",
    "                        stats.kurtosis(dat[colname], nan_policy='omit'),\n",
    "                        100*np.sum(np.where(np.isnan(dat[colname])))/self.n_records\n",
    "                    ])\n",
    "                    summary_data[idfts, icolname*7:icolname*7 + 7] = summary_row\n",
    "\n",
    "            self.summary = pd.DataFrame(summary_data, columns=summary_header)\n",
    "            self.summary['TIMESTAMP'] = desired_file_tss\n",
    "            self.summary.set_index('TIMESTAMP', inplace=True)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9b0c5f5-24d2-4ef3-9e42-088d9005266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dirs = [\"/Volumes/TempData/Bretfeld Mario/Chimney-Park-Reprocessing-Sandbox/Alex Work/Bad/Chimney/EC Processing/BB-NF/Fast/17m/Converted\"]\n",
    "metadata_fn = \"/Volumes/TempData/Bretfeld Mario/Chimney/Site Information/Changelog_Alex_fieldnotes.xlsx\"\n",
    "file_length = 30\n",
    "acq_freq = 10\n",
    "start_time = \"2021-03-11 00:00\"\n",
    "end_time = \"2021-03-11 11:00\"\n",
    "site_names = [\"BB-NF17\"]\n",
    "out_dir = \"/Volumes/TempData/Bretfeld Mario/Chimney-Park-Reprocessing-Sandbox/Alex Work/Bad/Chimney/EC Processing/BB-NF/Fast/17m/Standardized\"\n",
    "\n",
    "processor = fast_processing_engine(converted_dirs, metadata_fn, file_length, acq_freq, start_time, end_time, site_names, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2646c7f8-7368-40ee-892d-9c76bdd91457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BB-NF17\n"
     ]
    }
   ],
   "source": [
    "processor.find_fast_files()\n",
    "processor.metadata_template()\n",
    "processor.get_fast_header_info()\n",
    "a=processor.site_info['BB-NF17']['header_metadat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242efc4-1215-4b14-9336-80635238a83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
