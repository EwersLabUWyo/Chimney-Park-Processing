{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f253b0a3-b111-4b8a-89ab-7a088354a2e8",
   "metadata": {},
   "source": [
    "# Converted Fast File Standardization\n",
    "\n",
    "This step is to be completed after following the CardConvert of the protocol outlined in Protocol.md\n",
    "\n",
    "The first thing that this notebook does is take the output files from CardConvert and process them into a more uniform format for EddyPro.\n",
    "\n",
    "In EddyPro, the following things need to be consistent across time:\n",
    "* Data column order\n",
    "* Number of header lines\n",
    "* Number of \"text\" columns to be ignored\n",
    "* Seamless boundaries between files\n",
    "\n",
    "However, EddyPro can handle the following, which will be addressed later:\n",
    "* Changes in instrument location and orientation\n",
    "* Changes in instrument type\n",
    "* Changes in acquisiton frequency and file size\n",
    "\n",
    "The program will also combine data from the same site. For example, at the BB-NF site, \"Fast\" data is collected at both a 3m tower and at a 17m tower. This data will have to be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f9c07d-7135-4210-8089-e13e8b0f72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d085eda-b5f1-494b-8454-59fa1be9a49b",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "\n",
    "Please input the following information in the next cell:\n",
    "* path to the directory containing the \"Converted\" files of interest\n",
    "* the time-span represented by each file, in minutes (usually 30)\n",
    "* the acquisition frequency of the files, in Hz (usually 10)\n",
    "* the start date of the timeseries, in yyyy-mm-dd HH:MM format. This should be the timestamp of the **first** line of the **first** file of interest, rounded **down** to the nearest half-hour.\n",
    "* the end date of the timeseries, in yyyy-mm-dd HH:MM format. This should be the timestamp of the **first** line of the **last** file of interest, rounded **down** to the nearest half-hour.\n",
    "* The name of the site (either BB-SF, BB-NF or BB-UF, or something else that the system recognizes). This will point the program towards the correct metadata file containing information on instruments and history for the site.\n",
    "\n",
    "The program will use this information to correctly format the \"Fast\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f94166e0-428c-432a-a750-46b6bc8a9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input: Change me!\n",
    "\n",
    "converted_dir = \"/Volumes/TempData/Bretfeld Mario/Chimney-Park-Reprocessing-Sandbox/Alex Work/Bad/Chimney/EC Processing/BB-NF/Fast/3m/Converted\"\n",
    "file_length_in = 30  # minutes\n",
    "file_length_out = 30  # minutes. Must be a divisor of file_length_in. Keep at 30/30 for simplicity.\n",
    "acq_freq = 10  # Hz\n",
    "start_time = \"2021-03-06 15:30\"  # yyyy-mm-dd HH:MM\n",
    "end_time = \"2021-03-07 11:00\"#\"2021-03-30 11:00\"\n",
    "site_name = \"BB-NF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1161ce4-d6aa-49cf-8114-750e4c9c2f0b",
   "metadata": {},
   "source": [
    "## Processing steps\n",
    "Run, but do not edit!\n",
    "\n",
    "For each time stamp in a given file, create an \"ideal\" file index, then merge the two, keeping everything in the \"ideal\" file and populating the other with nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7459f1de-2a06-4b44-9f42-498a8bd5b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_from_raw():\n",
    "    def __init__(self, converted_dir, file_length_in, file_length_out, acq_freq, start_time, end_time, site_name):\n",
    "        self.start_time = pd.to_datetime(start_time)\n",
    "        self.end_time = pd.to_datetime(end_time)\n",
    "\n",
    "        # convert acq freq to interval\n",
    "        self.acq_freq = acq_freq\n",
    "        self.acq_period = pd.Timedelta(f'{1000//self.acq_freq} ms')\n",
    "    \n",
    "        self.file_length = file_length\n",
    "        self.file_length_out = file_length_out\n",
    "        self.n_records = self.file_length*self.acq_freq*60\n",
    "        \n",
    "        self.site_name = site_name\n",
    "\n",
    "        # convert files and directories to path objects and create an output directory\n",
    "        self.converted_path = Path(converted_dir)\n",
    "        self.out_path = self.converted_path / \"..\" / \"Standardized\"\n",
    "        if not self.out_path.exists():\n",
    "            self.out_path.mkdir()\n",
    "        return\n",
    "    \n",
    "    def get_timestamp_from_fn(self, fn):\n",
    "        \"\"\"given a file, this will extract the timestamp in its name\"\"\"\n",
    "        file_id = Path(fn).name.split('Hz')[1]\n",
    "        file_start_str = \"\".join(re.split(\"_|\\.\", file_id)[1:-1])\n",
    "        file_start_ts = pd.to_datetime(file_start_str, format=\"%Y%m%d%H%M\")\n",
    "        return file_start_ts\n",
    "\n",
    "    def get_fn_from_timestamp(self, file_start_ts, fns_lst):\n",
    "        \"\"\"given a timestamp, this will find the exact file name it's associated with\"\"\"\n",
    "        file_start_str = (f'{file_start_ts.year:04d}_' + \n",
    "                          f'{file_start_ts.month:02d}_' + \n",
    "                          f'{file_start_ts.day:02d}_' + \n",
    "                          f'{file_start_ts.hour:02d}{file_start_ts.minute:02d}')\n",
    "\n",
    "        # if the file exists\n",
    "        for i, fn in enumerate(fns_lst):\n",
    "            if file_start_str in str(fn):\n",
    "                return fns_lst[i]\n",
    "\n",
    "        # if not\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    def find_files(self):\n",
    "        \"\"\"find all the raw data files that the user wants to process\"\"\"\n",
    "        \n",
    "        start_time, end_time, file_length, site_name, converted_path = self.start_time, self.end_time, self.file_length, self.site_name, self.converted_path\n",
    "        \n",
    "        desired_file_tss = pd.date_range(start_time, end_time, freq=f'{file_length} min')\n",
    "        fns = list(converted_path.glob(\"TOA5*.dat\"))  # list all files\n",
    "\n",
    "        file_tss = []\n",
    "        for fn in fns:\n",
    "            fts = self.get_timestamp_from_fn(fn)\n",
    "            if (fts >= start_time and fts <= end_time):\n",
    "                file_tss.append(self.get_timestamp_from_fn(fn))\n",
    "\n",
    "        file_tss = sorted(file_tss)\n",
    "        fns_sorted = [self.get_fn_from_timestamp(fts, fns) for fts in file_tss]  # use that to sort the file names\n",
    "        fns = fns_sorted.copy()\n",
    "        file_ts_dict = {ts:fn for ts, fn in zip(file_tss, fns)}  # also a dict is nice I guess\n",
    "        n_files_converted = len(fns)\n",
    "        \n",
    "\n",
    "        self.desired_file_tss, self.file_tss, self.fns, self.file_ts_dict, self.n_files_converted = desired_file_tss, file_tss, fns, file_ts_dict, n_files_converted\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def metadata_template(self):\n",
    "        \"\"\"create a blank template to store raw file metadata in\"\"\"\n",
    "        n_files_converted = self.n_files_converted\n",
    "        rawfile_metadata = pd.DataFrame(\n",
    "            data=np.zeros((n_files_converted, 8), dtype=str),\n",
    "            columns=['Encoding', 'Station_name', 'Datalogger_model', 'Datalogger_serial_number', 'Datalogger_OS_version', 'Datalogger_program_name', 'Datalogger_program_signature', 'Table_name']\n",
    "        )\n",
    "        rawfile_metadata['File_name'] = self.fns\n",
    "        rawfile_metadata['TIMESTAMP'] = self.file_tss\n",
    "        rawfile_metadata['Start_fn'] = np.NAN\n",
    "        rawfile_metadata.set_index('TIMESTAMP', inplace=True)\n",
    "\n",
    "        self.rawfile_metadata = rawfile_metadata\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def summary_template(self, header):\n",
    "        # initialize summary statistics array: [TIMESTAMP, RECORD, Ux_Max, Ux_Min, Ux_Std, ... rho_c_Max, ...]\n",
    "        summary_header = []\n",
    "        for colname in header[2:]:\n",
    "            summary_header.append(colname + '_Max')\n",
    "            summary_header.append(colname + '_Min')\n",
    "            summary_header.append(colname + '_Std')\n",
    "            summary_header.append(colname + '_Mean')\n",
    "            summary_header.append(colname + '_NANPct')\n",
    "            summary_header.append(colname + '_Skw')\n",
    "            summary_header.append(colname + '_Krt')\n",
    "        summary_data = np.empty((len(self.desired_file_tss), len(summary_header)))\n",
    "        return summary_data, summary_header\n",
    "        \n",
    "    \n",
    "    def get_header(self, site_name):\n",
    "        \"\"\"each site has a specially formatted header associated with it\"\"\"\n",
    "        if site_name == 'BB-NF3':\n",
    "            return [\"TIMESTAMP\",\"RECORD\",\"Ux_CSAT3B\",\"Uy_CSAT3B\",\"Uz_CSAT3B\",\"Ts_CSAT3B\",\"rho_c_LI7500\",\"rho_v_LI7500\",\"P_LI7500\",\"DIAG_LI7500\"]\n",
    "        elif site_name == 'BB-NF17':\n",
    "            return [\"TIMESTAMP\",\"RECORD\",\"Ux_CSAT3_17m\",\"Uy_CSAT3_17m\",\"Uz_CSAT3_17m\",\"Ts_CSAT3_17m\",\"Ux_CSAT3_7m\",\"Uy_CSAT3_7m\",\"Uz_CSAT3_7m\",\"Ts_CSAT3_7m\",\"rho_c_LI7500\",\"rho_v_LI7500\",\"P_LI7500\",\"DIAG_LI7500\"]\n",
    "        \n",
    "    def write_out(self, dfts, dat):\n",
    "        # generate output file name: yyyy_mm_dd_HHMM.csv\n",
    "        \n",
    "        # number of out files to write\n",
    "        out_per_in = self.file_length//self.file_length_out\n",
    "        # number of entries per out file\n",
    "        n_records_out = self.n_records//out_per_in\n",
    "        # timestamps of out files\n",
    "        out_tss = pd.date_range(dfts, periods=out_per_in, freq=f'{file_length_out} min')\n",
    "        \n",
    "        # generate file names and write to out dir\n",
    "        for i, ots in zip(range(0, self.n_records, n_records_out), out_tss):\n",
    "            ots_str = re.sub('-| ', '_', str(ots))\n",
    "            ots_str = re.sub(':', '', ots_str)[:-2]\n",
    "            out_fn = self.out_path / f'{ots_str}.csv'\n",
    "            out_dat = dat.iloc[i:i + n_records_out]\n",
    "            dat.to_csv(out_fn, sep=',', na_rep='NAN', index=False, index_label=False, line_terminator='\\n')\n",
    "        \n",
    "    def standardize_fast_files(self):\n",
    "        \"\"\"reads in raw fast files, and combines/standardizes them to be continuous. Re-writes the files. Also computes diagnostic/summary statistics on the data\"\"\"\n",
    "\n",
    "        fns, file_tss, file_ts_dict, desired_file_tss, acq_period, n_records, out_path, file_length = self.fns, self.file_tss, self.file_ts_dict, self.desired_file_tss, self.acq_period, self.n_records, self.out_path, self.file_length\n",
    "        \n",
    "        # each site has a unique header\n",
    "        header = self.get_header(self.site_name)\n",
    "        \n",
    "        # initialize summary data\n",
    "        summary_data, summary_header = self.summary_template(header)\n",
    "        \n",
    "        # we'll be popping file names off of fns and file_tss, so we'll create temporary copies of them first\n",
    "        fns_temp, file_tss_temp = fns.copy(), file_tss.copy()\n",
    "        \n",
    "        # start processing files by desired start timestamp:\n",
    "        # for each file time window (usually every half-hour), find all the converted raw files that will fit into that time window.\n",
    "        # then, if there are any holes in the timeseries defined by those raw files, fill the missing timestamps and then fill the missing data\n",
    "        # with NANs. Write the final file to a csv and report file summary statistics.\n",
    "        for idfts, dfts in enumerate(tqdm(desired_file_tss)):\n",
    "            # generate the desired time index of the input file (start 100ms after file timestamp)\n",
    "            desired_time_index = pd.date_range(dfts + acq_period, periods=n_records, freq=acq_period)\n",
    "\n",
    "            # create an empty array containing the full desired index\n",
    "            dat = pd.DataFrame(desired_time_index, columns=['TIMESTAMP'])\n",
    "            dat.set_index('TIMESTAMP', inplace=True)\n",
    "\n",
    "            # search for files within the range covered by this file and load into an array indexed by timestamp\n",
    "            next_file_ts = dfts + pd.Timedelta(f'{file_length} Min')\n",
    "\n",
    "            next_fns, next_file_tss = [], []\n",
    "            # when the list empties, the while loop throws in index error.\n",
    "            try:\n",
    "                while file_tss_temp[0] < next_file_ts:\n",
    "                    next_fns.append(fns_temp.pop(0))\n",
    "                    next_file_tss.append(file_tss_temp.pop(0))\n",
    "            except IndexError as err:\n",
    "                pass\n",
    "            \n",
    "            # combine the found files\n",
    "            # if no valid files are found, just make a null dataframe\n",
    "            if next_fns == []:\n",
    "                rawdat = pd.DataFrame(np.full((1, len(header)), np.nan), columns = header)\n",
    "                rawdat['TIMESTAMP'] = desired_time_index[0]\n",
    "                rawdat.set_index('TIMESTAMP', inplace=True)\n",
    "            # otherwise proceed as normal\n",
    "            else:\n",
    "                for i, fn, ts in zip(range(len(next_fns)), next_fns, next_file_tss):\n",
    "                    if i == 0:\n",
    "                        rawdat = pd.read_csv(fn, sep=',', header=[0], skiprows=[0, 2, 3], na_values = ['NAN', '-4400906'])\n",
    "                        rawdat['TIMESTAMP'] = pd.to_datetime(rawdat['TIMESTAMP'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                        rawdat.set_index('TIMESTAMP', inplace=True)\n",
    "                    else:\n",
    "                        rawdat_tmp = pd.read_csv(fn, sep=',', header=[0], skiprows=[0, 2, 3], na_values = ['NAN', '-4400906'])\n",
    "                        rawdat_tmp['TIMESTAMP'] = pd.to_datetime(rawdat_tmp['TIMESTAMP'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                        rawdat_tmp.set_index('TIMESTAMP', inplace=True)\n",
    "                        rawdat = pd.concat([rawdat, rawdat_tmp])\n",
    "                        rawdat = rawdat.merge(rawdat_tmp, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "                    # get the metadata\n",
    "                    dfts_str = re.sub('-| ', '_', str(dfts))\n",
    "                    dfts_str = re.sub(':', '', dfts_str)[:-2]\n",
    "                    desired_fn = out_path / f'{dfts_str}.csv'\n",
    "                    with open(fn) as f: self.rawfile_metadata.loc[ts] = f.readline()[1:-2].split('\",\"') + [fn, desired_fn]\n",
    "\n",
    "            # merge raw files into complete array\n",
    "            dat = dat.merge(rawdat, how='outer', left_index=True, right_index=True, sort=True)\n",
    "            self.write_out(dfts, dat)\n",
    "            \n",
    "            \n",
    "            # write summary stats\n",
    "            for icolname, colname in enumerate(dat.columns[2:]):\n",
    "                summary_row = np.array([\n",
    "                    np.nanmax(dat[colname]),\n",
    "                    np.nanmin(dat[colname]),\n",
    "                    np.nanstd(dat[colname]),\n",
    "                    np.nanmean(dat[colname]),\n",
    "                    stats.skew(dat[colname], nan_policy='omit'),\n",
    "                    stats.kurtosis(dat[colname], nan_policy='omit'),\n",
    "                    100*np.sum(np.where(np.isnan(dat[colname])))/self.n_records\n",
    "                ])\n",
    "                summary_data[idfts, icolname*7:icolname*7 + 7] = summary_row\n",
    "        \n",
    "        self.summary = pd.DataFrame(summary_data, columns=summary_header)\n",
    "        self.summary['TIMESTAMP'] = desired_file_tss\n",
    "        self.summary.set_index('TIMESTAMP', inplace=True)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def process(self):\n",
    "        self.find_files()\n",
    "        self.metadata_template()\n",
    "        self.standardize_fast_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baa32f63-0c18-4a09-9563-21e27b9b1244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:18<00:00,  1.22it/s]\n",
      "100%|██████████| 40/40 [00:23<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "dirs = [\n",
    "    \"/Volumes/TempData/Bretfeld Mario/Chimney-Park-Reprocessing-Sandbox/Alex Work/Bad/Chimney/EC Processing/BB-NF/Fast/17m/Converted\",\n",
    "    \"/Volumes/TempData/Bretfeld Mario/Chimney-Park-Reprocessing-Sandbox/Alex Work/Bad/Chimney/EC Processing/BB-NF/Fast/3m/Converted\"\n",
    "]\n",
    "\n",
    "file_length_in = 30  # minutes\n",
    "file_length_out = 30  # minutes\n",
    "acq_freq = 10  # Hz\n",
    "start_times = [\"2021-03-11 00:00\", \"2021-03-06 15:30\"]  # yyyy-mm-dd HH:MM\n",
    "end_times = [\"2021-03-11 11:00\", \"2021-03-07 11:00\"]  # 3/30\n",
    "site_names = [\"BB-NF17\", \"BB-NF3\"]\n",
    "\n",
    "\n",
    "for converted_dir, start_time, end_time, site_name in zip(dirs, start_times, end_times, site_names):\n",
    "    processing_engine = process_from_raw(converted_dir, file_length_in, file_length_out, acq_freq, start_time, end_time, site_name)\n",
    "    processing_engine.process()\n",
    "    summary = processing_engine.summary\n",
    "    summary.to_csv(processing_engine.converted_path / '../Converted_Summary.csv')\n",
    "    meta = processing_engine.rawfile_metadata\n",
    "    meta.to_csv(processing_engine.converted_path / '../Converted_Metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa9274-7126-4736-83fc-4a3f558df169",
   "metadata": {},
   "source": [
    "Initialize the processing engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc204e1-7de0-4809-8b1e-e77301607209",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "0. Aggregate data from every site simultaneously and automate gap filling\n",
    "1. Implement processing for slow files\n",
    "2. Combine data between BBSF 4m/7m and between BBNF 3m/17m\n",
    "3. Adapt to site changes detailed in the site_changes.txt file when processing data and headers\n",
    "4. Adapt to mistakes in logger code\n",
    "5. Gap fill between neighboring sites for things like Precip, airtemp, etc.\n",
    "6. Apply calibrations\n",
    "\n",
    "Extra: try to find ideal in/out size ratio\n",
    "* 30min/30min: 3it/s\n",
    "* 60min/30min:\n",
    "* 120min/30min:\n",
    "* 1440min/30min:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf627c8-91eb-408b-ab9b-327d544acfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
